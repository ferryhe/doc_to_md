#!/usr/bin/env python3
"""
Engine Comparison Benchmark Tool

This script tests and compares the performance of different document conversion engines,
measuring conversion time, output characteristics (Markdown length and asset count), 
and success rates.
"""
from __future__ import annotations

import json
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

# Ensure project root is importable
PROJECT_ROOT = Path(__file__).resolve().parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from config.settings import EngineName, get_settings
from doc_to_md.cli import ENGINE_REGISTRY, ENGINES_REQUIRING_MODEL
from doc_to_md.engines.base import Engine, EngineResponse


@dataclass
class EngineResult:
    """Result from testing a single engine"""
    engine_name: str
    model: str
    success: bool
    conversion_time: float
    markdown_length: int = 0
    num_assets: int = 0
    error_message: str | None = None


@dataclass
class BenchmarkResult:
    """Complete benchmark test results"""
    timestamp: str
    test_file: str
    file_size_bytes: int
    results: list[EngineResult] = field(default_factory=list)
    
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary format"""
        return {
            'timestamp': self.timestamp,
            'test_file': self.test_file,
            'file_size_bytes': self.file_size_bytes,
            'results': [asdict(r) for r in self.results]
        }


class EngineBenchmark:
    """Engine benchmark testing class"""
    
    def __init__(self, engines_to_test: list[tuple[EngineName, str | None]] | None = None):
        """
        Initialize benchmark testing
        
        Args:
            engines_to_test: List of engines to test, format: [(engine_name, model), ...]
                           If None, will test all available engines
        """
        self.settings = get_settings()
        
        if engines_to_test is None:
            # Default: test all engines
            self.engines_to_test = [
                ("local", None),
                ("markitdown", None),
                ("paddleocr", None),
                ("docling", None),
                ("marker", None),
                ("mineru", None),
                ("mistral", self.settings.mistral_default_model),
                ("deepseekocr", self.settings.siliconflow_default_model),
            ]
        else:
            self.engines_to_test = engines_to_test
    
    def _create_engine(self, engine_name: EngineName, model: str | None) -> tuple[Engine | None, str | None]:
        """
        Create engine instance
        
        Returns:
            Tuple of (engine_instance, error_message). If creation succeeds, error_message is None.
        """
        try:
            engine_cls = ENGINE_REGISTRY.get(engine_name)
            if engine_cls is None:
                return None, f"Engine '{engine_name}' not found in registry"
            
            # Some engines require a model parameter
            if engine_name in ENGINES_REQUIRING_MODEL:
                return engine_cls(model=model), None
            return engine_cls(), None
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            print(f"‚ùå Failed to create engine {engine_name}: {error_msg}")
            return None, error_msg
    
    def test_engine(self, engine_name: EngineName, model: str | None, test_file: Path) -> EngineResult:
        """
        Test a single engine
        
        Args:
            engine_name: Engine name
            model: Model name (optional)
            test_file: Test file path
        
        Returns:
            EngineResult: Test results
        """
        print(f"\nüìä Testing engine: {engine_name} (model: {model or 'default'})")
        
        # Create engine instance
        engine, create_error = self._create_engine(engine_name, model)
        if engine is None:
            return EngineResult(
                engine_name=engine_name,
                model=model or "default",
                success=False,
                conversion_time=0.0,
                error_message=create_error or "Failed to create engine instance (missing dependencies or configuration)"
            )
        
        # Execute conversion and measure time
        try:
            start_time = time.time()
            response: EngineResponse = engine.convert(test_file)
            conversion_time = time.time() - start_time
            
            # Collect results
            result = EngineResult(
                engine_name=engine_name,
                model=response.model,
                success=True,
                conversion_time=conversion_time,
                markdown_length=len(response.markdown),
                num_assets=len(response.assets)
            )
            
            print(f"‚úÖ Success - Time: {conversion_time:.2f}s, Markdown length: {result.markdown_length}, Assets: {result.num_assets}")
            return result
            
        except Exception as e:
            conversion_time = time.time() - start_time
            error_msg = f"{type(e).__name__}: {str(e)}"
            print(f"‚ùå Failed - {error_msg}")
            return EngineResult(
                engine_name=engine_name,
                model=model or "default",
                success=False,
                conversion_time=conversion_time,
                error_message=error_msg
            )
    
    def run_benchmark(self, test_file: Path) -> BenchmarkResult:
        """
        Run complete benchmark test
        
        Args:
            test_file: Test file path
        
        Returns:
            BenchmarkResult: Complete test results
        """
        print(f"\n{'='*60}")
        print(f"Starting benchmark test")
        print(f"Test file: {test_file}")
        print(f"File size: {test_file.stat().st_size / 1024:.2f} KB")
        print(f"{'='*60}")
        
        benchmark_result = BenchmarkResult(
            timestamp=datetime.now().isoformat(),
            test_file=str(test_file),
            file_size_bytes=test_file.stat().st_size
        )
        
        for engine_name, model in self.engines_to_test:
            result = self.test_engine(engine_name, model, test_file)
            benchmark_result.results.append(result)
        
        return benchmark_result


class ChineseReportGenerator:
    """Chinese comparison report generator"""
    
    def __init__(self, benchmark_result: BenchmarkResult):
        self.result = benchmark_result
    
    def _format_time(self, seconds: float) -> str:
        """Format time"""
        if seconds < 0.01:
            return "< 0.01Áßí"
        return f"{seconds:.2f}Áßí"
    
    def _format_size(self, bytes_size: int) -> str:
        """Format file size"""
        if bytes_size < 1024:
            return f"{bytes_size} B"
        elif bytes_size < 1024 * 1024:
            return f"{bytes_size / 1024:.2f} KB"
        else:
            return f"{bytes_size / (1024 * 1024):.2f} MB"
    
    def _get_rating(self, time: float, success: bool) -> str:
        """Get performance rating"""
        if not success:
            return "‚ùå Â§±Ë¥•"
        if time < 5:
            return "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‰ºòÁßÄ"
        elif time < 15:
            return "‚≠ê‚≠ê‚≠ê‚≠ê ËâØÂ•Ω"
        elif time < 30:
            return "‚≠ê‚≠ê‚≠ê ‰∏ÄËà¨"
        elif time < 60:
            return "‚≠ê‚≠ê ËæÉÊÖ¢"
        else:
            return "‚≠ê ÁºìÊÖ¢"
    
    def generate_markdown_report(self) -> str:
        """Generate Chinese comparison report in Markdown format"""
        lines = []
        
        # Title and basic information
        lines.append("# ÊñáÊ°£ËΩ¨Êç¢ÂºïÊìéÂØπÊØîÊµãËØïÊä•Âëä")
        lines.append("")
        lines.append("## ÊµãËØï‰ø°ÊÅØ")
        lines.append("")
        lines.append(f"- **ÊµãËØïÊó∂Èó¥**: {self.result.timestamp}")
        lines.append(f"- **ÊµãËØïÊñá‰ª∂**: `{self.result.test_file}`")
        lines.append(f"- **Êñá‰ª∂Â§ßÂ∞è**: {self._format_size(self.result.file_size_bytes)}")
        lines.append(f"- **ÊµãËØïÂºïÊìéÊï∞Èáè**: {len(self.result.results)}")
        lines.append("")
        
        # Success rate statistics
        successful = sum(1 for r in self.result.results if r.success)
        failed = len(self.result.results) - successful
        success_rate = (successful / len(self.result.results) * 100) if self.result.results else 0
        
        lines.append("## Êï¥‰ΩìÁªüËÆ°")
        lines.append("")
        lines.append(f"- **ÊàêÂäü**: {successful} ‰∏™ÂºïÊìé")
        lines.append(f"- **Â§±Ë¥•**: {failed} ‰∏™ÂºïÊìé")
        lines.append(f"- **ÊàêÂäüÁéá**: {success_rate:.1f}%")
        lines.append("")
        
        # Performance rankings
        successful_results = [r for r in self.result.results if r.success]
        if successful_results:
            lines.append("## ÊÄßËÉΩÊéíÂêçÔºàÊåâËΩ¨Êç¢Êó∂Èó¥Ôºâ")
            lines.append("")
            sorted_results = sorted(successful_results, key=lambda r: r.conversion_time)
            for i, result in enumerate(sorted_results, 1):
                lines.append(f"{i}. **{result.engine_name}** ({result.model})")
                lines.append(f"   - ËΩ¨Êç¢Êó∂Èó¥: {self._format_time(result.conversion_time)}")
                lines.append(f"   - ËæìÂá∫ÈïøÂ∫¶: {result.markdown_length:,} Â≠óÁ¨¶")
                lines.append(f"   - ËµÑÊ∫êÊï∞Èáè: {result.num_assets}")
                lines.append("")
        
        # Detailed test results table
        lines.append("## ËØ¶ÁªÜÊµãËØïÁªìÊûú")
        lines.append("")
        lines.append("| ÂºïÊìéÂêçÁß∞ | Ê®°Âûã | Áä∂ÊÄÅ | ËΩ¨Êç¢Êó∂Èó¥ | MarkdownÈïøÂ∫¶ | ËµÑÊ∫êÊï∞ | ÊÄßËÉΩËØÑÁ∫ß |")
        lines.append("|---------|------|------|---------|------------|-------|---------|")
        
        for result in self.result.results:
            status = "‚úÖ ÊàêÂäü" if result.success else "‚ùå Â§±Ë¥•"
            time_str = self._format_time(result.conversion_time) if result.success else "-"
            md_len = f"{result.markdown_length:,}" if result.success else "-"
            assets = str(result.num_assets) if result.success else "-"
            rating = self._get_rating(result.conversion_time, result.success)
            
            lines.append(
                f"| {result.engine_name} | {result.model} | {status} | "
                f"{time_str} | {md_len} | {assets} | {rating} |"
            )
        
        lines.append("")
        
        # Error information
        failed_results = [r for r in self.result.results if not r.success]
        if failed_results:
            lines.append("## Â§±Ë¥•ËØ¶ÊÉÖ")
            lines.append("")
            for result in failed_results:
                lines.append(f"### {result.engine_name} ({result.model})")
                lines.append("")
                lines.append("```")
                lines.append(result.error_message or "Êú™Áü•ÈîôËØØ")
                lines.append("```")
                lines.append("")
        
        # Engine characteristics analysis
        lines.append("## ÂºïÊìéÁâπÁÇπÂàÜÊûê")
        lines.append("")
        
        engine_descriptions = {
            "local": {
                "name": "Êú¨Âú∞ÂºïÊìé",
                "pros": ["Êó†ÈúÄÂ§ñÈÉ®‰æùËµñ", "Âø´ÈÄüÁÆÄÂçï", "ÈÄÇÂêàÁ∫ØÊñáÊú¨ÊñáÊ°£", "Êó†ÁΩëÁªúËØ∑Ê±Ç"],
                "cons": ["OCRËÉΩÂäõÊúâÈôê", "Ê†ºÂºèÊîØÊåÅËæÉÂ∞ë", "ËæìÂá∫Ë¥®Èáè‰∏ÄËà¨"],
                "best_for": "Á∫ØÊñáÊú¨Êñá‰ª∂„ÄÅÂø´ÈÄüÊµãËØï„ÄÅÁ¶ªÁ∫øÁéØÂ¢É"
            },
            "mistral": {
                "name": "Mistral OCR",
                "pros": ["Âº∫Â§ßÁöÑOCRËÉΩÂäõ", "ÊîØÊåÅÂõæÂÉèÊèêÂèñ", "È´òË¥®ÈáèËæìÂá∫", "ÊîØÊåÅÂ§öÁßçÊ†ºÂºè"],
                "cons": ["ÈúÄË¶ÅAPIÂØÜÈí•", "ÊúâÁΩëÁªúÂª∂Ëøü", "ÂèØËÉΩÊúâÊàêÊú¨", "‰æùËµñÂ§ñÈÉ®ÊúçÂä°"],
                "best_for": "Â§çÊùÇPDFÊñáÊ°£„ÄÅÈ´òË¥®ÈáèOCRÈúÄÊ±Ç„ÄÅÊúâÈ¢ÑÁÆóÁöÑÈ°πÁõÆ"
            },
            "deepseekocr": {
                "name": "DeepSeek OCR",
                "pros": ["‰ºòÁßÄÁöÑ‰∏≠ÊñáOCR", "ËßÜËßâÁêÜËß£ËÉΩÂäõÂº∫", "ÊîØÊåÅÂ§çÊùÇÂ∏ÉÂ±Ä", "ËæìÂá∫Ë¥®ÈáèÈ´ò"],
                "cons": ["ÈúÄË¶ÅAPIÂØÜÈí•", "ÁΩëÁªú‰æùËµñ", "Â§ÑÁêÜÈÄüÂ∫¶ÂèØËÉΩËæÉÊÖ¢"],
                "best_for": "‰∏≠ÊñáÊñáÊ°£„ÄÅÂ§çÊùÇÊéíÁâà„ÄÅÈúÄË¶ÅÈ´òÁ≤æÂ∫¶ÁöÑÂú∫ÊôØ"
            },
            "markitdown": {
                "name": "MarkItDown",
                "pros": ["ÊîØÊåÅÂ§öÁßçOfficeÊ†ºÂºè", "Êú¨Âú∞Â§ÑÁêÜ", "Âø´ÈÄü", "‰øùÁúüÂ∫¶È´ò"],
                "cons": ["ÂØπÊâ´ÊèèPDFÊîØÊåÅÊúâÈôê", "ÈúÄË¶ÅÂÆâË£Ö‰æùËµñ", "Êèí‰ª∂ÂèØËÉΩ‰∏çÁ®≥ÂÆö"],
                "best_for": "OfficeÊñáÊ°£ÔºàDOCX„ÄÅPPTX„ÄÅXLSXÔºâ„ÄÅÊú¨Âú∞Â§ÑÁêÜÈúÄÊ±Ç"
            },
            "paddleocr": {
                "name": "PaddleOCR",
                "pros": ["Êú¨Âú∞OCR", "ÊîØÊåÅÂ§öËØ≠Ë®Ä", "ÂèØGPUÂä†ÈÄü", "ÂºÄÊ∫êÂÖçË¥π"],
                "cons": ["ÈúÄË¶ÅÂ§ßÈáè‰æùËµñ", "È¶ñÊ¨°‰ΩøÁî®ÈúÄ‰∏ãËΩΩÊ®°Âûã", "ÂáÜÁ°ÆÂ∫¶‰∏≠Á≠â"],
                "best_for": "Êú¨Âú∞OCRÈúÄÊ±Ç„ÄÅÊâπÈáèÂ§ÑÁêÜ„ÄÅ‰∏≠ÊñáÊñáÊ°£"
            },
            "docling": {
                "name": "Docling",
                "pros": ["IBM‰ºÅ‰∏öÁ∫ßÊñπÊ°à", "ÁªìÊûÑÂåñÊèêÂèñ", "ÊîØÊåÅÂ§çÊùÇÊñáÊ°£", "È´òË¥®ÈáèËæìÂá∫"],
                "cons": ["‰æùËµñËæÉÈáç", "Â§ÑÁêÜÈÄüÂ∫¶ËæÉÊÖ¢", "ÈÖçÁΩÆÂ§çÊùÇ"],
                "best_for": "‰ºÅ‰∏öÊñáÊ°£„ÄÅÁªìÊûÑÂåñÊèêÂèñ„ÄÅÈúÄË¶ÅÈ´òË¥®ÈáèÁöÑÂú∫ÊôØ"
            },
            "marker": {
                "name": "Marker",
                "pros": ["‰∏ìÊ≥®PDFËΩ¨Êç¢", "‰øùÁïôÊ†ºÂºè", "ÊîØÊåÅLLMÂ¢ûÂº∫", "ÂõæÂÉèÊèêÂèñ"],
                "cons": ["‰æùËµñÈáç", "ÂèØËÉΩÈúÄË¶ÅGPU", "Â§ÑÁêÜÈÄüÂ∫¶‰∏ÄËà¨"],
                "best_for": "Â≠¶ÊúØËÆ∫Êñá„ÄÅÂ§çÊùÇPDF„ÄÅÈúÄË¶Å‰øùÁïôÊ†ºÂºè"
            },
            "mineru": {
                "name": "MinerU",
                "pros": ["Â§öÁßçËß£ÊûêÊñπÊ≥ï", "GPUÂä†ÈÄü", "ÊîØÊåÅÂ§çÊùÇÂ∏ÉÂ±Ä", "ÂºÄÊ∫ê"],
                "cons": ["‰æùËµñÈáç", "ÈÖçÁΩÆÂ§çÊùÇ", "ËµÑÊ∫êÊ∂àËÄóÂ§ß"],
                "best_for": "Á†îÁ©∂Áî®ÈÄî„ÄÅÊâπÈáèÂ§ÑÁêÜ„ÄÅÊúâGPUÁéØÂ¢É"
            }
        }
        
        for result in self.result.results:
            if result.engine_name in engine_descriptions:
                desc = engine_descriptions[result.engine_name]
                lines.append(f"### {desc['name']} (`{result.engine_name}`)")
                lines.append("")
                
                status_emoji = "‚úÖ" if result.success else "‚ùå"
                lines.append(f"**ÊµãËØïÁä∂ÊÄÅ**: {status_emoji} {'ÊàêÂäü' if result.success else 'Â§±Ë¥•'}")
                lines.append("")
                
                lines.append("**‰ºòÁÇπ**:")
                for pro in desc["pros"]:
                    lines.append(f"- {pro}")
                lines.append("")
                
                lines.append("**Áº∫ÁÇπ**:")
                for con in desc["cons"]:
                    lines.append(f"- {con}")
                lines.append("")
                
                lines.append(f"**ÊúÄÈÄÇÂêà**: {desc['best_for']}")
                lines.append("")
        
        # Usage recommendations
        lines.append("## ‰ΩøÁî®Âª∫ËÆÆ")
        lines.append("")
        lines.append("Ê†πÊçÆÊµãËØïÁªìÊûúÔºåÊàë‰ª¨Êèê‰æõ‰ª•‰∏ã‰ΩøÁî®Âª∫ËÆÆÔºö")
        lines.append("")
        
        if successful_results:
            fastest = min(successful_results, key=lambda r: r.conversion_time)
            lines.append(f"1. **ÈÄüÂ∫¶‰ºòÂÖà**: ‰ΩøÁî® `{fastest.engine_name}` ÂºïÊìéÔºà{self._format_time(fastest.conversion_time)}Ôºâ")
            lines.append("")
            
            # Find the engine with longest output (usually means most detailed)
            longest = max(successful_results, key=lambda r: r.markdown_length)
            lines.append(f"2. **ËæìÂá∫ËØ¶ÁªÜÂ∫¶‰ºòÂÖà**: ‰ΩøÁî® `{longest.engine_name}` ÂºïÊìéÔºàËæìÂá∫ {longest.markdown_length:,} Â≠óÁ¨¶Ôºâ")
            lines.append("")
        
        lines.append("3. **ÊàêÊú¨ËÄÉËôë**:")
        lines.append("   - ÂÖçË¥πÊñπÊ°à: `local`, `markitdown`, `paddleocr`, `docling`, `marker`, `mineru`")
        lines.append("   - ‰ªòË¥πÊñπÊ°à: `mistral`, `deepseekocr`ÔºàÈúÄË¶ÅAPIÂØÜÈí•Ôºâ")
        lines.append("")
        
        lines.append("4. **ÊñáÊ°£Á±ªÂûãÂª∫ËÆÆ**:")
        lines.append("   - Á∫ØÊñáÊú¨/ÁÆÄÂçïÊñáÊ°£: `local` Êàñ `markitdown`")
        lines.append("   - OfficeÊñáÊ°£: `markitdown`")
        lines.append("   - Êâ´ÊèèPDF/ÂõæÁâá: `mistral`, `deepseekocr`, Êàñ `paddleocr`")
        lines.append("   - Â§çÊùÇÂ≠¶ÊúØPDF: `marker` Êàñ `docling`")
        lines.append("   - ‰∏≠ÊñáÊñáÊ°£: `deepseekocr` Êàñ `paddleocr`")
        lines.append("")
        
        # Conclusion
        lines.append("## ÁªìËÆ∫")
        lines.append("")
        lines.append(f"Êú¨Ê¨°ÊµãËØïÂÖ±ËØÑ‰º∞‰∫Ü {len(self.result.results)} ‰∏™ÊñáÊ°£ËΩ¨Êç¢ÂºïÊìéÔºå"
                    f"ÊàêÂäüÁéá‰∏∫ {success_rate:.1f}%„ÄÇ")
        lines.append("")
        
        if successful_results:
            lines.append("ÊØè‰∏™ÂºïÊìéÈÉΩÊúâÂÖ∂ÁâπÂÆöÁöÑ‰ºòÂäøÂíåÈÄÇÁî®Âú∫ÊôØ„ÄÇÈÄâÊã©ÂêàÈÄÇÁöÑÂºïÊìéÈúÄË¶ÅÁªºÂêàËÄÉËôëÔºö")
            lines.append("")
            lines.append("- ÊñáÊ°£Á±ªÂûãÂíåÂ§çÊùÇÂ∫¶")
            lines.append("- Â§ÑÁêÜÈÄüÂ∫¶Ë¶ÅÊ±Ç")
            lines.append("- ËæìÂá∫ËØ¶ÁªÜÂ∫¶Ë¶ÅÊ±Ç")
            lines.append("- ÊàêÊú¨È¢ÑÁÆó")
            lines.append("- ÊòØÂê¶ÈúÄË¶ÅÁ¶ªÁ∫øÂ§ÑÁêÜ")
            lines.append("- ËØ≠Ë®ÄÊîØÊåÅÔºàÁâπÂà´ÊòØ‰∏≠ÊñáÔºâ")
        else:
            lines.append("‚ö†Ô∏è ÊâÄÊúâÂºïÊìéÈÉΩÂ§±Ë¥•‰∫Ü„ÄÇËØ∑Ê£ÄÊü•Ôºö")
            lines.append("- ÊµãËØïÊñá‰ª∂ÊòØÂê¶ÊúâÊïà")
            lines.append("- ÂøÖË¶ÅÁöÑ‰æùËµñÊòØÂê¶Â∑≤ÂÆâË£Ö")
            lines.append("- APIÂØÜÈí•ÊòØÂê¶Â∑≤ÈÖçÁΩÆÔºàÂØπ‰∫éËøúÁ®ãÂºïÊìéÔºâ")
        
        lines.append("")
        lines.append("---")
        lines.append(f"*Êä•ÂëäÁîüÊàêÊó∂Èó¥: {self.result.timestamp}*")
        
        return "\n".join(lines)
    
    def save_report(self, output_path: Path) -> None:
        """Save report to file"""
        report = self.generate_markdown_report()
        output_path.write_text(report, encoding="utf-8")
        print(f"\n‚úÖ Report saved to: {output_path}")


def create_sample_test_file(output_dir: Path) -> Path:
    """Create sample test file"""
    output_dir.mkdir(parents=True, exist_ok=True)
    test_file = output_dir / "sample_test.txt"
    
    content = """# Test Document - Document Conversion Test

This is a sample file for testing document conversion engines.

## Features

1. Multiple engine support
2. Performance comparison
3. Quality assessment

## Tech Stack

- Python 3.10+
- Typer (CLI framework)
- Pydantic (Configuration management)
- Multiple OCR/ML engines

## Mixed Chinese-English Content

Document conversion is a complex task that requires considering multiple factors:

1. Accuracy
2. Speed
3. Cost
4. Format support

Test content includes plain text, special characters (@#$%^&*), numbers (123456), etc.
"""
    
    test_file.write_text(content, encoding="utf-8")
    return test_file


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Engine Comparison Benchmark Tool"
    )
    parser.add_argument(
        "--test-file",
        type=Path,
        help="Path to test file (will create a sample if not provided)"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("benchmark_results"),
        help="Output directory (default: benchmark_results)"
    )
    parser.add_argument(
        "--engines",
        type=str,
        nargs="+",
        help="List of engines to test (e.g., local mistral deepseekocr)"
    )
    parser.add_argument(
        "--save-json",
        action="store_true",
        help="Also save results in JSON format"
    )
    
    args = parser.parse_args()
    
    # Prepare test file
    if args.test_file and args.test_file.exists():
        test_file = args.test_file
        print(f"Using test file: {test_file}")
    elif args.test_file:
        # User provided a file path but it doesn't exist - this is an error
        print(f"Error: Test file '{args.test_file}' does not exist")
        sys.exit(1)
    else:
        # No test file provided, create a sample
        print("Creating sample test file...")
        test_file = create_sample_test_file(args.output_dir)
        print(f"Sample test file created: {test_file}")
    
    # Prepare engine list
    engines_to_test = None
    if args.engines:
        settings = get_settings()
        engines_to_test = []
        for engine_name in args.engines:
            if engine_name in {"mistral"}:
                engines_to_test.append((engine_name, settings.mistral_default_model))
            elif engine_name in {"deepseekocr"}:
                engines_to_test.append((engine_name, settings.siliconflow_default_model))
            else:
                engines_to_test.append((engine_name, None))
        print(f"Will test engines: {', '.join(args.engines)}")
    else:
        print("Will test all available engines")
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Run benchmark test
    benchmark = EngineBenchmark(engines_to_test)
    result = benchmark.run_benchmark(test_file)
    
    # Generate report
    print(f"\n{'='*60}")
    print("Generating Chinese comparison report...")
    print(f"{'='*60}")
    
    generator = ChineseReportGenerator(result)
    
    # Save Markdown report
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = args.output_dir / f"comparison_report_{timestamp}.md"
    generator.save_report(report_path)
    
    # Optional: Save JSON format
    if args.save_json:
        json_path = args.output_dir / f"benchmark_result_{timestamp}.json"
        json_path.write_text(
            json.dumps(result.to_dict(), ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"‚úÖ JSON results saved to: {json_path}")
    
    print(f"\n{'='*60}")
    print("Benchmark test completed!")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
